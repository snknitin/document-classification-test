{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=os.path.join(os.getcwd(),\"data/shuffled-full-set-hashed.csv\")\n",
    "dataframe_all = pd.read_csv(data_file, sep=\",\")\n",
    "dataframe_all.columns = [\"document_label\", \"word_values\"]\n",
    "dataframe_all=dataframe_all.dropna(subset = ['word_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"\n",
      "/root/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "for train_index, test_index in split.split(dataframe_all['word_values'], dataframe_all['document_label']):\n",
    "    strat_train_set = dataframe_all.loc[train_index]\n",
    "    strat_test_set = dataframe_all.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set=strat_train_set.dropna(subset = ['word_values'])\n",
    "strat_test_set=strat_test_set.dropna(subset = ['word_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train=strat_train_set['word_values'], strat_train_set['document_label']\n",
    "x_test, y_test=strat_test_set['word_values'], strat_test_set['document_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "label_encoder = LabelEncoder()\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "class LabelEncoderPipelineFriendly(LabelEncoder):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n",
    "        super(LabelEncoderPipelineFriendly, self).fit(X)\n",
    "    def transform(self, X, y=None):\n",
    "        return super(LabelEncoderPipelineFriendly, self).transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(LabelEncoderPipelineFriendly, self).fit(X).transform(X)\n",
    "\n",
    "class ArrayCaster(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return np.transpose(np.matrix(data))\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector('word_values')),\n",
    "        ('vect', TfidfVectorizer(max_df=0.95, min_df=2, max_features=1037929)),\n",
    "        ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector('document_label')),\n",
    "        ('encoder', LabelEncoderPipelineFriendly()),\n",
    "    ('caster', ArrayCaster()),\n",
    "    ]) \n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "          #(\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train=full_pipeline.fit_transform(strat_train_set), label_encoder.fit_transform(strat_train_set['document_label'])\n",
    "x_test, y_test=full_pipeline.transform(strat_test_set), label_encoder.fit_transform(strat_test_set['document_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#the outcome (dependent variable) has only a limited number of possible values. \n",
    "#Logistic Regression is used when response variable is categorical in nature.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#A random forest is a meta estimator that fits a number of decision tree classifiers \n",
    "#on various sub-samples of the dataset and use averaging to improve the predictive \n",
    "#accuracy and control over-fitting.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#a discriminative classifier formally defined by a separating hyperplane.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#for measuring training time\n",
    "from time import time \n",
    "# F1 score (also F-score or F-measure) is a measure of a test's accuracy. \n",
    "#It considers both the precision p and the recall r of the test to compute \n",
    "#the score: p is the number of correct positive results divided by the number of \n",
    "#all positive results, and r is the number of correct positive results divided by \n",
    "#the number of positive results that should have been returned. The F1 score can be \n",
    "#interpreted as a weighted average of the precision and recall, where an F1 score \n",
    "#reaches its best value at 1 and worst at 0.\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    \n",
    "    end = time()\n",
    "    # Print and return results\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    \n",
    "    return f1_score(target, y_pred,labels=range(14),average='micro'), sum(target == y_pred) / float(len(y_pred))\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    #print \"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    f1, acc = predict_labels(clf, X_train, y_train)\n",
    "    print f1, acc\n",
    "    print \"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc)\n",
    "    \n",
    "    f1, acc = predict_labels(clf, X_test, y_test)\n",
    "    print \"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_A = LogisticRegression(C=14, solver='lbfgs',multi_class='multinomial',random_state = 42)\n",
    "# train_predict(clf_A, x_train, y_train, x_test, y_test)\n",
    "# print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Trained model in 1774.9372 seconds\n",
      "Made predictions in 6.9654 seconds.\n",
      "0.881493274184 0.881493274184\n",
      "F1 score and accuracy score for training set: 0.8815 , 0.8815.\n",
      "Made predictions in 0.8239 seconds.\n",
      "F1 score and accuracy score for test set: 0.8755 , 0.8755.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the three models \n",
    "print \"Start\"\n",
    "# clf_B = SVC(random_state = 912, kernel='rbf')\n",
    "#Boosting refers to this general problem of producing a very accurate prediction rule \n",
    "#by combining rough and moderately inaccurate rules-of-thumb\n",
    "clf_C = xgb.XGBClassifier(seed = 82)\n",
    "\n",
    "\n",
    "# train_predict(clf_B, x_train, y_train, x_test, y_test)\n",
    "# print ''\n",
    "train_predict(clf_C, x_train, y_train, x_test, y_test)\n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/root/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = { 'learning_rate' : [0.1],\n",
    "               'n_estimators' : [40],\n",
    "               'max_depth': [3],\n",
    "               'min_child_weight': [3],\n",
    "               'gamma':[0.4],\n",
    "               'subsample' : [0.8],\n",
    "               'colsample_bytree' : [0.8],\n",
    "               'scale_pos_weight' : [1],\n",
    "               'reg_alpha':[1e-5]\n",
    "             }  \n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = xgb.XGBClassifier(seed=2)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score,labels=range(14),average='micro')\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf,\n",
    "                        scoring=f1_scorer,\n",
    "                        param_grid=parameters,\n",
    "                        cv=5)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(x_train,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "print clf\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "f1, acc = predict_labels(clf, x_train, y_train)\n",
    "print \"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc)\n",
    "    \n",
    "f1, acc = predict_labels(clf, x_test, y_test)\n",
    "print \"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
